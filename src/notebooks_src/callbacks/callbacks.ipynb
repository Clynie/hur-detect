{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from configs.ipynb\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: './logs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5535a75adbd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnbfinder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNotebookFinder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNotebookFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconfigs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/evan/hur-detect/src/notebooks_src/callbacks/nbfinder.pyc\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(self, fullname)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_transformer_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;31m# run the code in themodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_user_ns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/evan/hur-detect/src/notebooks_src/callbacks/configs.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: './logs'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "from configs import configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WeightPrinter(keras.callbacks.Callback):\n",
    "\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        weights = self.model.get_weights()\n",
    "        for i, weight_set in enumerate(weights):\n",
    "\n",
    "            weights_norm = np.sum(np.square(weight_set))\n",
    "            min_weight = np.min(weight_set)\n",
    "            max_weight = np.max(weight_set)\n",
    "            weight_shape = weight_set.shape\n",
    "            sys.stderr.write(\"\\n ####### \\n Weight Set %i shape: %s\\nweight norm: %5.2f \\nmin_weight: %5.2f\\nmax_weight: %5.2f\\n ##### \\n\"%(i, str(weight_shape),\n",
    "                                                                                                               weights_norm,\n",
    "                                                                                                               min_weight,\n",
    "                                                                                                               max_weight))\n",
    "        sys.stderr.write(str(logs.get(\"loss\")))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LearnCurve(keras.callbacks.Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.glob_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        #sys.stderr.write(str(logs))\n",
    "        self.losses.append(logs.get('loss'))\n",
    "    def on_epoch_end(self,epoch, logs):\n",
    "        self.glob_losses.append(logs.get('loss'))\n",
    "        if epoch % 1 ==0:\n",
    "\n",
    "            plt.plot(self.glob_losses)\n",
    "            plt.show()\n",
    "        \n",
    "        \n",
    "\n",
    "    def on_train_end(self,*args):\n",
    "        plt.plot(self.glob_losses)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sched(epoch_ind):\n",
    "    if epoch_ind < 50:\n",
    "        return 0.0001\n",
    "    elif epoch_ind < 100:\n",
    "        return 0.00001\n",
    "    else:\n",
    "        return 0.000001\n",
    "\n",
    "lr_sched_cback = keras.callbacks.LearningRateScheduler(schedule=sched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TensorBoard(keras.callbacks.Callback):\n",
    "    \"\"\"Tensorboard basic visualizations.\n",
    "    This callback writes a log for TensorBoard, which allows\n",
    "    you to visualize dynamic graphs of your training and test\n",
    "    metrics, as well as activation histograms for the different\n",
    "    layers in your model.\n",
    "    TensorBoard is a visualization tool provided with TensorFlow.\n",
    "    If you have installed TensorFlow with pip, you should be able\n",
    "    to launch TensorBoard from the command line:\n",
    "    ```\n",
    "    tensorboard --logdir=/full_path_to_your_logs\n",
    "    ```\n",
    "    You can find more information about TensorBoard\n",
    "    [here](https://www.tensorflow.org/versions/master/how_tos/summaries_and_tensorboard/index.html).\n",
    "    # Arguments\n",
    "        log_dir: the path of the directory where to save the log\n",
    "            files to be parsed by Tensorboard\n",
    "        histogram_freq: frequency (in epochs) at which to compute activation\n",
    "            histograms for the layers of the model. If set to 0,\n",
    "            histograms won't be computed.\n",
    "        write_graph: whether to visualize the graph in Tensorboard.\n",
    "            The log file can become quite large when\n",
    "            write_graph is set to True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, log_dir='./logs',\n",
    "                 histogram_freq=0,\n",
    "                 write_graph=True,\n",
    "                 write_images=False):\n",
    "        super(TensorBoard, self).__init__()\n",
    "        if K.backend() != 'tensorflow':\n",
    "            raise RuntimeError('TensorBoard callback only works '\n",
    "                               'with the TensorFlow backend.')\n",
    "        self.log_dir = log_dir\n",
    "        self.histogram_freq = histogram_freq\n",
    "        self.merged = None\n",
    "        self.write_graph = write_graph\n",
    "        self.write_images = write_images\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        self.sess = K.get_session()\n",
    "        if self.histogram_freq and self.merged is None:\n",
    "            for layer in self.model.layers:\n",
    "\n",
    "                for weight in layer.weights:\n",
    "\n",
    "                    tf.summary.histogram(weight.name, weight)\n",
    "\n",
    "                    if self.write_images:\n",
    "                        w_img = tf.squeeze(weight)\n",
    "                        \n",
    "\n",
    "                        shape = w_img.get_shape()\n",
    "                        sys.stderr.write(str(shape) + \"\\n\")\n",
    "                        if len(shape) > 1 and shape[0] > shape[1]:\n",
    "                            w_img = tf.transpose(w_img)\n",
    "\n",
    "                        if len(shape) == 1:\n",
    "                            w_img = tf.expand_dims(w_img, 0)\n",
    "\n",
    "                        w_img = tf.expand_dims(tf.expand_dims(w_img, 0), -1)\n",
    "\n",
    "\n",
    "                        tf.summary.image(weight.name, w_img)\n",
    "\n",
    "                if hasattr(layer, 'output'):\n",
    "\n",
    "                    tf.summary.histogram('{}_out'.format(layer.name),\n",
    "                                             layer.output)\n",
    "\n",
    "      \n",
    "        self.merged = tf.summary.merge_all()\n",
    "\n",
    "        if self.write_graph:\n",
    "            if hasattr(tf, 'summary') and hasattr(tf.summary, 'FileWriter'):\n",
    "                self.writer = tf.summary.FileWriter(self.log_dir,\n",
    "                                                    self.sess.graph)\n",
    "            elif parse_version(tf.__version__) >= parse_version('0.8.0'):\n",
    "                self.writer = tf.train.SummaryWriter(self.log_dir,\n",
    "                                                     self.sess.graph)\n",
    "            else:\n",
    "                self.writer = tf.train.SummaryWriter(self.log_dir,\n",
    "                                                     self.sess.graph_def)\n",
    "        else:\n",
    "            if hasattr(tf, 'summary') and hasattr(tf.summary, 'FileWriter'):\n",
    "                self.writer = tf.summary.FileWriter(self.log_dir)\n",
    "            else:\n",
    "                self.writer = tf.train.SummaryWriter(self.log_dir)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        \n",
    "        if self.model.validation_data and self.histogram_freq:\n",
    "            if epoch % self.histogram_freq == 0:\n",
    "                # TODO: implement batched calls to sess.run\n",
    "                # (current call will likely go OOM on GPU)\n",
    "                if self.model.uses_learning_phase:\n",
    "                    cut_v_data = len(self.model.inputs)\n",
    "                    val_data = self.model.validation_data[:cut_v_data] + [0]\n",
    "                    tensors = self.model.inputs + [K.learning_phase()]\n",
    "                else:\n",
    "                    val_data = self.model.validation_data\n",
    "                    tensors = self.model.inputs\n",
    "                feed_dict = dict(zip(tensors, val_data))\n",
    "                result = self.sess.run([self.merged], feed_dict=feed_dict)\n",
    "                summary_str = result[0]\n",
    "                self.writer.add_summary(summary_str, epoch)\n",
    "\n",
    "        for name, value in logs.items():\n",
    "            sys.stderr.write(name + \"\\n\")\n",
    "            if name in ['batch', 'size']:\n",
    "                continue\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.writer.add_summary(summary, epoch)\n",
    "        self.writer.flush()\n",
    "\n",
    "    def on_train_end(self, _):\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TB = TensorBoard(log_dir='./logs/'+ configs[\"experiment_name\"],\n",
    "                                            histogram_freq=1, write_graph=False, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_callbacks():\n",
    "    return [LearnCurve(), TB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
