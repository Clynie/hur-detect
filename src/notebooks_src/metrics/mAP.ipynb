{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "import tensorflow as tf\n",
    "if __name__ == \"__main__\":\n",
    "    sys.path.append(\"../../\")\n",
    "from notebooks_src.losses.util import unpack_net_output\n",
    "from notebooks_src.box_encode_decode.ssd.encode import encode, encode_one_fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_accuracy_for_one_fmap(y_true, y_pred):\n",
    "    \"\"\"on one batch\"\"\"\n",
    "    \n",
    "    feat_shape = tuple([d.value for d in list(y_pred.get_shape()[1:3])])\n",
    "\n",
    "    gclasses, glocalizations, gscores = encode_one_fmap(y_true, feat_shape=feat_shape)\n",
    "\n",
    "\n",
    "    localizations, logits = unpack_net_output(y_pred, feat_shape,single_example=False)\n",
    "    \n",
    "    localisations = ssd_net.bboxes_decode(localisations, ssd_anchors)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_shape = [1] * 5 + [len(ssd_anchors)] * 3\n",
    "\n",
    "\n",
    "# Make a batch of data\n",
    "r = tf.train.batch(\n",
    "    tf_utils.reshape_list([image, glabels, gbboxes, gdifficults, gbbox_img,\n",
    "                           gclasses, glocalisations, gscores]),\n",
    "    batch_size=FLAGS.batch_size,\n",
    "    num_threads=FLAGS.num_preprocessing_threads,\n",
    "    capacity=5 * FLAGS.batch_size,\n",
    "    dynamic_pad=True)\n",
    "(b_image, b_glabels, b_gbboxes, b_gdifficults, b_gbbox_img, b_gclasses,\n",
    " b_glocalisations, b_gscores) = tf_utils.reshape_list(r, batch_shape)\n",
    "\n",
    "# =================================================================== #\n",
    "# SSD Network + Ouputs decoding.\n",
    "# =================================================================== #\n",
    "dict_metrics = {}\n",
    "arg_scope = ssd_net.arg_scope(data_format=DATA_FORMAT)\n",
    "# 1. get outputs from network\n",
    "# localisations is the box corrdinates\n",
    "# logits is the raw output of the convolution for classes (ie unnormalized prob dist)\n",
    "# predictions is the softmax of the logits\n",
    "predictions, localisations, logits, end_points = \\\n",
    "    ssd_net.net(b_image, is_training=False)\n",
    "\n",
    "\n",
    "# Performing post-processing on CPU: loop-intensive, usually more efficient.\n",
    "with tf.device('/device:CPU:0'):\n",
    "    \n",
    "    \n",
    "# decode encoded output for normalization to box coords?\n",
    "localisations = ssd_net.bboxes_decode(localisations, ssd_anchors)\n",
    "\n",
    "\n",
    "# get top k  predicted boxes after nms\n",
    "rscores, rbboxes = \\\n",
    "    ssd_net.detected_bboxes(predictions, localisations,\n",
    "                            select_threshold=FLAGS.select_threshold,\n",
    "                            nms_threshold=FLAGS.nms_threshold,\n",
    "                            clipping_bbox=None,\n",
    "                            top_k=FLAGS.select_top_k,\n",
    "                            keep_top_k=FLAGS.keep_top_k)\n",
    "    \n",
    "    \n",
    "#match the predicted boxes to ground truth boxes and compute the TP and FP statistics.\n",
    "num_gbboxes, tp, fp, rscores = \\\n",
    "    tfe.bboxes_matching_batch(rscores.keys(), rscores, rbboxes,\n",
    "                              b_glabels, b_gbboxes, b_gdifficults,\n",
    "                              matching_threshold=FLAGS.matching_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tp_fp_metric = tfe.streaming_tp_fp_arrays(num_gbboxes, tp, fp, rscores)\n",
    "            for c in tp_fp_metric[0].keys():\n",
    "                dict_metrics['tp_fp_%s' % c] = (tp_fp_metric[0][c],\n",
    "                                                tp_fp_metric[1][c])\n",
    "\n",
    "            # Add to summaries precision/recall values.\n",
    "            aps_voc07 = {}\n",
    "            aps_voc12 = {}\n",
    "            for c in tp_fp_metric[0].keys():\n",
    "                # Precison and recall values.\n",
    "                prec, rec = tfe.precision_recall(*tp_fp_metric[0][c])\n",
    "\n",
    "                # Average precision VOC07.\n",
    "                v = tfe.average_precision_voc07(prec, rec)\n",
    "                summary_name = 'AP_VOC07/%s' % c\n",
    "                op = tf.summary.scalar(summary_name, v, collections=[])\n",
    "                # op = tf.Print(op, [v], summary_name)\n",
    "                tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\n",
    "                aps_voc07[c] = v\n",
    "\n",
    "                # Average precision VOC12.\n",
    "                v = tfe.average_precision_voc12(prec, rec)\n",
    "                summary_name = 'AP_VOC12/%s' % c\n",
    "                op = tf.summary.scalar(summary_name, v, collections=[])\n",
    "                # op = tf.Print(op, [v], summary_name)\n",
    "                tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\n",
    "                aps_voc12[c] = v\n",
    "\n",
    "            # Mean average precision VOC07.\n",
    "            summary_name = 'AP_VOC07/mAP'\n",
    "            mAP = tf.add_n(list(aps_voc07.values())) / len(aps_voc07)\n",
    "            op = tf.summary.scalar(summary_name, mAP, collections=[])\n",
    "            op = tf.Print(op, [mAP], summary_name)\n",
    "            tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\n",
    "\n",
    "            # Mean average precision VOC12.\n",
    "            summary_name = 'AP_VOC12/mAP'\n",
    "            mAP = tf.add_n(list(aps_voc12.values())) / len(aps_voc12)\n",
    "            op = tf.summary.scalar(summary_name, mAP, collections=[])\n",
    "            op = tf.Print(op, [mAP], summary_name)\n",
    "            tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class streaming_acc_class(object):\n",
    "    def __init__(self, inp mode=\"VOC\"):\n",
    "        #intialize tensors for calculating one batch of acc\n",
    "        pass\n",
    "        \n",
    "    def update_acc(self,x,y):\n",
    "        #do a forward pass with feed dict and get tp/fp for each class and append to the master ones\n",
    "        pass\n",
    "    def get_final_acc(self):\n",
    "        # do final prec_recall on each class and return the average as well\n",
    "        \n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
