{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ../../notebooks_src/models/configs.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/config_util.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/load_data/configs.ipynb\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "if __name__ == \"__main__\":\n",
    "    sys.path.append(\"../../\")\n",
    "from notebooks_src.box_encode_decode.configs import configs\n",
    "from notebooks_src.models.configs import configs as model_configs\n",
    "configs.update(model_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_epsilon(tens):\n",
    "    epsilon = tf.constant(value=0.0001)\n",
    "    return tf.add(tens,epsilon)\n",
    "    \n",
    "\n",
    "def average_nonzero_elements(tens):\n",
    "    denominator = tf.count_nonzero(tens)\n",
    "    denominator = tf.cast(denominator, tf.float32) + 0.001\n",
    "    tens_sum = tf.reduce_sum(tens)\n",
    "    mean_tens = tf.div(tens_sum, denominator)\n",
    "    return mean_tens\n",
    "    \n",
    "\n",
    "\n",
    "def mask_tens(tens, mask):\n",
    "    return tf.mul(tens, mask)   \n",
    "    #tf.where(tf.equal(mask, tf.zeros_like(mask)),tf.zeros_like(tens),  tens)\n",
    "    \n",
    "\n",
    "def zero_out_nans(tens):\n",
    "    return tf.where(condition=tf.is_nan(tens),x=tf.zeros_like(tens), y=tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpack_net_output(y_preds):\n",
    "    locs,logs,preds = [], [], []\n",
    "    for y_pred in y_preds:\n",
    "        feat_shape = tuple([d.value for d in list(y_pred.get_shape()[1:3])])\n",
    "        loc,log,pred = _unpack_net_output(y_pred, feat_shape)\n",
    "        locs.append(loc)\n",
    "        logs.append(log)\n",
    "        preds.append(pred)\n",
    "    \n",
    "    return locs,logs,preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _unpack_net_output(pred_tensor, feat_shape, single_example=False):\n",
    "    num_classes = configs[\"num_classes\"]\n",
    "    ind = configs[\"feat_shapes\"].index(feat_shape)\n",
    "    sizes, ratios = configs[\"anchor_sizes\"][ind], configs[\"anchor_ratios\"][ind]\n",
    "    num_boxes = len(sizes)+ len(ratios)\n",
    "    num_coords_per_box = 4\n",
    "    loc_channels = num_boxes * num_coords_per_box\n",
    "    cls_channels = num_classes * num_boxes\n",
    "    \"\"\"split by grabbing the first loc_channels channels\n",
    "    these will be concatenated like so:\n",
    "     * localizations (num_ex, ydim,xdim,loc_channels) on top of\n",
    "     * logits (num_ex,ydim,xdim,cls_channels)\n",
    "     so we split by the dim 3 (counting up from 0)\n",
    "     then we reshape each to be: \n",
    "        * (num_ex, ydim,xdim,num_boxes, num_coords_per_box)\n",
    "        * (num_ex, ydim,xdim,num_boxes, num_classes) \"\"\"\n",
    "    \n",
    "    if single_example:\n",
    "        cutoff_axis = 2\n",
    "    else:\n",
    "        cutoff_axis = 3\n",
    "    localizations, logits = tf.split(pred_tensor, axis=cutoff_axis,num_or_size_splits=[loc_channels, cls_channels])\n",
    "    #print localizations.get_shape(), logits.get_shape()\n",
    "    localizations_shape = convert_tf_shape_to_int_tuple(localizations.get_shape())\n",
    "    new_localization_shape = tuple(list(localizations_shape[0:cutoff_axis]) + [num_boxes, num_coords_per_box])\n",
    "    localizations = tf.reshape(localizations, shape=new_localization_shape)\n",
    "    \n",
    "    logits_shape = convert_tf_shape_to_int_tuple(logits.get_shape())\n",
    "    new_logits_shape = tuple(list(logits_shape[0:cutoff_axis]) + [num_boxes, num_classes])\n",
    "    logits = tf.reshape(logits, shape=new_logits_shape)\n",
    "    \n",
    "    predictions = tf.contrib.slim.softmax(logits)\n",
    "    \n",
    "    \n",
    "                               \n",
    "    return localizations, logits, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_int_tensor_shape(tensor):\n",
    "    return convert_tf_shape_to_int_tuple(tensor.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_tf_shape_to_int_tuple(tf_shape):\n",
    "    return tuple([dim.value for dim in tf_shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape_key(tensor):\n",
    "    return get_int_tensor_shape(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_some_lists_of_tensors(*lists):\n",
    "    #print lists\n",
    "    return [sorted(tensor_list,key=shape_key) for tensor_list in lists]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2015 Paul Balanca. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Implement some custom layers, not provided by TensorFlow.\n",
    "Trying to follow as much as possible the style/standards used in\n",
    "tf.contrib.layers\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.framework.python.ops import add_arg_scope\n",
    "from tensorflow.contrib.layers.python.layers import initializers\n",
    "from tensorflow.contrib.framework.python.ops import variables\n",
    "from tensorflow.contrib.layers.python.layers import utils\n",
    "from tensorflow.python.ops import nn\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import variable_scope\n",
    "\n",
    "\n",
    "def abs_smooth(x):\n",
    "    \"\"\"Smoothed absolute function. Useful to compute an L1 smooth error.\n",
    "    Define as:\n",
    "        x^2 / 2         if abs(x) < 1\n",
    "        abs(x) - 0.5    if abs(x) > 1\n",
    "    We use here a differentiable definition using min(x) and abs(x). Clearly\n",
    "    not optimal, but good enough for our purpose!\n",
    "    \"\"\"\n",
    "    absx = tf.abs(x)\n",
    "    minx = tf.minimum(absx, 1)\n",
    "    r = 0.5 * ((absx - 1) * minx + absx)\n",
    "    return r\n",
    "\n",
    "\n",
    "@add_arg_scope\n",
    "def l2_normalization(\n",
    "        inputs,\n",
    "        scaling=False,\n",
    "        scale_initializer=init_ops.ones_initializer(),\n",
    "        reuse=None,\n",
    "        variables_collections=None,\n",
    "        outputs_collections=None,\n",
    "        data_format='NHWC',\n",
    "        trainable=True,\n",
    "        scope=None):\n",
    "    \"\"\"Implement L2 normalization on every feature (i.e. spatial normalization).\n",
    "    Should be extended in some near future to other dimensions, providing a more\n",
    "    flexible normalization framework.\n",
    "    Args:\n",
    "      inputs: a 4-D tensor with dimensions [batch_size, height, width, channels].\n",
    "      scaling: whether or not to add a post scaling operation along the dimensions\n",
    "        which have been normalized.\n",
    "      scale_initializer: An initializer for the weights.\n",
    "      reuse: whether or not the layer and its variables should be reused. To be\n",
    "        able to reuse the layer scope must be given.\n",
    "      variables_collections: optional list of collections for all the variables or\n",
    "        a dictionary containing a different list of collection per variable.\n",
    "      outputs_collections: collection to add the outputs.\n",
    "      data_format:  NHWC or NCHW data format.\n",
    "      trainable: If `True` also add variables to the graph collection\n",
    "        `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "    Returns:\n",
    "      A `Tensor` representing the output of the operation.\n",
    "    \"\"\"\n",
    "\n",
    "    with variable_scope.variable_scope(\n",
    "            scope, 'L2Normalization', [inputs], reuse=reuse) as sc:\n",
    "        inputs_shape = inputs.get_shape()\n",
    "        inputs_rank = inputs_shape.ndims\n",
    "        dtype = inputs.dtype.base_dtype\n",
    "        if data_format == 'NHWC':\n",
    "            # norm_dim = tf.range(1, inputs_rank-1)\n",
    "            norm_dim = tf.range(inputs_rank-1, inputs_rank)\n",
    "            params_shape = inputs_shape[-1:]\n",
    "        elif data_format == 'NCHW':\n",
    "            # norm_dim = tf.range(2, inputs_rank)\n",
    "            norm_dim = tf.range(1, 2)\n",
    "            params_shape = (inputs_shape[1])\n",
    "\n",
    "        # Normalize along spatial dimensions.\n",
    "        outputs = nn.l2_normalize(inputs, norm_dim, epsilon=1e-12)\n",
    "        # Additional scaling.\n",
    "        if scaling:\n",
    "            scale_collections = utils.get_variable_collections(\n",
    "                variables_collections, 'scale')\n",
    "            scale = variables.model_variable('gamma',\n",
    "                                             shape=params_shape,\n",
    "                                             dtype=dtype,\n",
    "                                             initializer=scale_initializer,\n",
    "                                             collections=scale_collections,\n",
    "                                             trainable=trainable)\n",
    "            if data_format == 'NHWC':\n",
    "                outputs = tf.multiply(outputs, scale)\n",
    "            elif data_format == 'NCHW':\n",
    "                scale = tf.expand_dims(scale, axis=-1)\n",
    "                scale = tf.expand_dims(scale, axis=-1)\n",
    "                outputs = tf.multiply(outputs, scale)\n",
    "                # outputs = tf.transpose(outputs, perm=(0, 2, 3, 1))\n",
    "\n",
    "        return utils.collect_named_outputs(outputs_collections,\n",
    "                                           sc.original_name_scope, outputs)\n",
    "\n",
    "\n",
    "@add_arg_scope\n",
    "def pad2d(inputs,\n",
    "          pad=(0, 0),\n",
    "          mode='CONSTANT',\n",
    "          data_format='NHWC',\n",
    "          trainable=True,\n",
    "          scope=None):\n",
    "    \"\"\"2D Padding layer, adding a symmetric padding to H and W dimensions.\n",
    "    Aims to mimic padding in Caffe and MXNet, helping the port of models to\n",
    "    TensorFlow. Tries to follow the naming convention of `tf.contrib.layers`.\n",
    "    Args:\n",
    "      inputs: 4D input Tensor;\n",
    "      pad: 2-Tuple with padding values for H and W dimensions;\n",
    "      mode: Padding mode. C.f. `tf.pad`\n",
    "      data_format:  NHWC or NCHW data format.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(scope, 'pad2d', [inputs]):\n",
    "        # Padding shape.\n",
    "        if data_format == 'NHWC':\n",
    "            paddings = [[0, 0], [pad[0], pad[0]], [pad[1], pad[1]], [0, 0]]\n",
    "        elif data_format == 'NCHW':\n",
    "            paddings = [[0, 0], [0, 0], [pad[0], pad[0]], [pad[1], pad[1]]]\n",
    "        net = tf.pad(inputs, paddings, mode=mode)\n",
    "        return net\n",
    "\n",
    "\n",
    "@add_arg_scope\n",
    "def channel_to_last(inputs,\n",
    "                    data_format='NHWC',\n",
    "                    scope=None):\n",
    "    \"\"\"Move the channel axis to the last dimension. Allows to\n",
    "    provide a single output format whatever the input data format.\n",
    "    Args:\n",
    "      inputs: Input Tensor;\n",
    "      data_format: NHWC or NCHW.\n",
    "    Return:\n",
    "      Input in NHWC format.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(scope, 'channel_to_last', [inputs]):\n",
    "        if data_format == 'NHWC':\n",
    "            net = inputs\n",
    "        elif data_format == 'NCHW':\n",
    "            net = tf.transpose(inputs, perm=(0, 2, 3, 1))\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 144, 16) (96, 144, 16)\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    feat_shape = (96,144)\n",
    "\n",
    "    num_classes = configs[\"num_classes\"]\n",
    "    ind = configs[\"feat_shapes\"].index(feat_shape)\n",
    "    sizes, ratios = configs[\"anchor_sizes\"][ind], configs[\"anchor_ratios\"][ind]\n",
    "    num_boxes = len(sizes)+ len(ratios)\n",
    "    num_coords_per_box = 4\n",
    "    loc_channels = num_boxes * num_coords_per_box\n",
    "    cls_channels = num_classes * num_boxes\n",
    "\n",
    "\n",
    "    pred_tensor = tf.ones( (feat_shape[0],feat_shape[1],loc_channels + cls_channels))\n",
    "\n",
    "    pred_tensor.get_shape()\n",
    "\n",
    "    loc, log = unpack_net_output(pred_tensor,feat_shape, single_example=True)\n",
    "\n",
    "    #assert convert_tf_shape_to_int_tuple(loc.get_shape()) == (1,96,144,4,4)\n",
    "    #assert convert_tf_shape_to_int_tuple(log.get_shape()) == (1,96,144,4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
