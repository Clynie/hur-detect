{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ../../../notebooks_src/losses/utils.ipynb\n",
      "importing Jupyter notebook from ../../../notebooks_src/postprocessing/utils.ipynb\n",
      "importing Jupyter notebook from ../../../notebooks_src/box_processing/match.ipynb\n",
      "importing Jupyter notebook from ../../../notebooks_src/box_processing/tf_box_util.ipynb\n",
      "importing Jupyter notebook from ../../../notebooks_src/box_processing/make_anchors_orig.ipynb\n",
      "importing Jupyter notebook from ../../../notebooks_src/box_processing/encode.ipynb\n",
      "importing Jupyter notebook from ../../../notebooks_src/box_processing/unpack.ipynb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "slim=tf.contrib.slim\n",
    "if __name__ == \"__main__\":\n",
    "    sys.path.append(\"../../../\")\n",
    "from notebooks_src.losses.utils import abs_smooth as smooth_L1\n",
    "\n",
    "from notebooks_src.configs import configs\n",
    "from notebooks_src.postprocessing.utils import get_int_tensor_shape, sort_some_lists_of_tensors\n",
    "from notebooks_src.box_processing.match import match_boxes\n",
    "from notebooks_src.box_processing.encode import encode\n",
    "from notebooks_src.box_processing.unpack import unpack_net_output, split_box_class\n",
    "#from utils import ssd_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(y_true, y_preds):\n",
    "    '''y_true: the boxes Nx15x5 tensor\n",
    "       y_preds: a list of 7?  tensors of Nxfy x fx x k where k = 4*number of anchors + number_of_anchors*num_classes,\n",
    "           N is number of examples'''\n",
    "    \n",
    "    loss_inputs = get_loss_inputs(y_true, y_preds)\n",
    "    cls_losses = {fmap_size: calc_cls_loss(fmap_size, loss_inputs) for fmap_size in loss_inputs[\"fmap_sizes\"]}\n",
    "    loc_losses = {fmap_size: calc_loc_loss(fmap_size, loss_inputs) for fmap_size in loss_inputs[\"fmap_sizes\"]}\n",
    "    \n",
    "    num_matches_dict = {fmap_size: loss_inputs[\"match_masks\"][fmap_size][\"num_matches\"] \n",
    "                   for fmap_size in loss_inputs[\"fmap_sizes\"]}\n",
    "    \n",
    "    summary_for_all_fmaps(cls_losses, prefix=\"cls_loss\")\n",
    "    summary_for_all_fmaps(loc_losses, prefix=\"loc_loss\")\n",
    "    summary_for_all_fmaps(num_matches_dict, prefix=\"num_matches\")\n",
    "    \n",
    "    \n",
    "    loss = calc_loss(cls_losses,loc_losses, num_matches_dict)\n",
    "    return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summary_for_all_fmaps(scalar_dict, prefix=\"\"):\n",
    "    with tf.name_scope(prefix):\n",
    "        for (fy,fx),v in scalar_dict.iteritems():\n",
    "            tf.summary.scalar(\"%s_fmap_Size_%i_%i_\" %(prefix,fy,fx), v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss_inputs(y_true, y_preds):\n",
    "    bboxes, labels = split_box_class(y_true)\n",
    "    encoded_gt = encode(bboxes, labels)\n",
    "    match_masks = match_boxes(bboxes)\n",
    "    detections = unpack_net_output(y_preds)\n",
    "    keys = detections[\"logits\"].keys()\n",
    "    return dict(detections=detections, encoded_gt=encoded_gt, match_masks=match_masks, fmap_sizes=keys)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_loss(cls_losses, loc_losses, num_matches_dict):\n",
    "    losses = []\n",
    "    for fmap_size in cls_losses.keys():\n",
    "        loc_loss = loc_losses[fmap_size]\n",
    "        cls_loss = cls_losses[fmap_size]\n",
    "        num_matches = num_matches_dict[fmap_size]\n",
    "        match_coeff = get_match_coeff(num_matches=num_matches)\n",
    "        losses.append((loc_loss + cls_loss) * match_coeff)\n",
    "    total_loss = tf.add_n(losses) * (1./ configs[\"batch_size\"])\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_loc_loss(fmap_size, loss_input_dict):\n",
    "    localizations, encoded_boxes, x_mask = get_loc_loss_terms(fmap_size, loss_input_dict) \n",
    "    loc_loss = smooth_L1(encoded_boxes - localizations)\n",
    "    loc_loss = tf.boolean_mask(loc_loss, x_mask)\n",
    "    loc_loss = tf.reduce_sum(loc_loss)\n",
    "    return loc_loss\n",
    "\n",
    "\n",
    "\n",
    "def calc_cls_loss(fmap_size, loss_input_dict ):\n",
    "    clt = get_cls_loss_terms(fmap_size, loss_input_dict)\n",
    "    pos_xent = get_positive_xent_loss(clt)\n",
    "    neg_xent = get_negative_xent_loss(clt)\n",
    "    return pos_xent + neg_xent\n",
    "\n",
    "def get_match_coeff(num_matches):\n",
    "    match_coeff = tf.where(tf.equal(num_matches, 0), 0., tf.div(1., tf.cast(num_matches, dtype=tf.float32)))\n",
    "    return match_coeff\n",
    "\n",
    "\n",
    "def get_loc_loss_terms(fmap_size, loss_input_dict):\n",
    "    localizations = loss_input_dict[\"detections\"][\"localizations\"][fmap_size]\n",
    "    localizations = tf.expand_dims(localizations,axis=-1)\n",
    "    \n",
    "    num_coords_in_a_box = 4\n",
    "    x_mask = loss_input_dict[\"match_masks\"][fmap_size][\"x_mask\"]\n",
    "    x_mask = tf.stack(num_coords_in_a_box*[x_mask], axis=-2)\n",
    "    \n",
    "    encoded_boxes = loss_input_dict[\"encoded_gt\"][fmap_size][\"boxes\"]\n",
    "    \n",
    "    return localizations, encoded_boxes, x_mask\n",
    "    \n",
    "\n",
    "    \n",
    "def get_negative_xent_loss(clt):\n",
    "    encoded_labels = clt[\"encoded_labels\"]\n",
    "    num_negs = tf.cast(configs[\"negative_ratio\"] * clt[\"num_matches\"], dtype=tf.int32)\n",
    "    num_negs = tf.where(tf.equal(num_negs,0), configs[\"default_negatives\"], num_negs)\n",
    "\n",
    "    hard_neg_mask = make_hard_neg_mask(clt[\"predictions\"],\n",
    "                                       clt[\"fp_mask\"],\n",
    "                                       num_negs)\n",
    "    \n",
    "    neg_encoded_labels = tf.where(hard_neg_mask, configs[\"num_classes\"]*tf.ones_like(encoded_labels), encoded_labels)\n",
    "    \n",
    "    neg_xent_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=clt[\"logits\"], \n",
    "                                                                   labels=neg_encoded_labels)\n",
    "    neg_xent_loss = tf.boolean_mask(neg_xent_loss, hard_neg_mask)\n",
    "    neg_xent = tf.reduce_sum(neg_xent_loss)\n",
    "    \n",
    "    return neg_xent\n",
    "    \n",
    "\n",
    "def get_positive_xent_loss(clt):\n",
    "    logits, labels, x_mask = clt[\"logits\"],clt[\"encoded_labels\"],clt[\"x_mask\"]\n",
    "    xent_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, \n",
    "                                                               labels=labels)\n",
    "    xent_loss = tf.boolean_mask(xent_loss, x_mask)\n",
    "    pos_xent = tf.reduce_sum(xent_loss)\n",
    "    return pos_xent\n",
    "    \n",
    "def get_cls_loss_terms(fmap_size, loss_input_dict):\n",
    "    logits = loss_input_dict[\"detections\"][\"logits\"][fmap_size]\n",
    "    logits = tf.stack(configs[\"num_max_boxes\"]*[logits], axis=-2)\n",
    "    \n",
    "    predictions = loss_input_dict[\"detections\"][\"predictions\"][fmap_size]\n",
    "    encoded_labels = loss_input_dict[\"encoded_gt\"][fmap_size][\"labels\"]\n",
    "    mask_dict = loss_input_dict[\"match_masks\"][fmap_size]\n",
    "    x_mask, tp_mask, actual_gt_box_mask, num_matches = [mask_dict[k] for k in [\"x_mask\", \n",
    "                                                                               \"tp_mask\", \n",
    "                                                                               \"actual_gt_box_mask\", \n",
    "                                                                               \"num_matches\"]]\n",
    "    fp_mask = make_fp_mask(actual_gt_box_mask, tp_mask)\n",
    "    return dict(logits=logits, encoded_labels=encoded_labels, \n",
    "                x_mask=x_mask, fp_mask=fp_mask, num_matches=num_matches, predictions=predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_hard_neg_mask(pred, fp_mask, num_negs):\n",
    "    pred = tf.stack(configs[\"num_max_boxes\"]*[pred], axis=-2)\n",
    "    float_fp_mask = tf.cast(fp_mask, dtype=pred.dtype)\n",
    "    float_fp_mask = tf.expand_dims(float_fp_mask, axis=-1)\n",
    "    fp_pred = pred * float_fp_mask\n",
    "    last_axis = len(fp_pred.get_shape()) -1\n",
    "    \n",
    "    # bg class is the last element of logits\n",
    "    _, bg_fp_pred = tf.split(fp_pred, num_or_size_splits=[configs[\"num_classes\"], 1], axis=last_axis)\n",
    "    flat_bg_fp_pred = tf.reshape(bg_fp_pred, [-1])\n",
    "    \n",
    "    num_total_negs = flat_bg_fp_pred.get_shape()[0]\n",
    "\n",
    "    k = tf.where(num_negs > num_total_negs, num_total_negs, num_negs)\n",
    "    bg_confs, inds = tf.nn.top_k(flat_bg_fp_pred,k=k)\n",
    "    min_bg_conf = bg_confs[-1]\n",
    "    hard_neg_mask = tf.greater_equal(bg_fp_pred, min_bg_conf)\n",
    "    hard_neg_mask = tf.squeeze(hard_neg_mask, axis=-1)\n",
    "\n",
    "    return hard_neg_mask\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_fp_mask(actual_gt_box_mask, tp_mask):\n",
    "    #is an actual box, but does not have an overlap > 0.5 with gt\n",
    "    fp_mask = tf.logical_and(actual_gt_box_mask, tf.logical_not(tp_mask))\n",
    "    fp_mask = tf.transpose(fp_mask, perm=(3,0,1,2,4))\n",
    "    #fp_mask = tf.Print(data=[tf.reduce_sum(tf.cast(fp_mask, dtype=tf.int32))], input_=fp_mask, message=\"fpmask num nonzeros: \")\n",
    "    return fp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "76.1715\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import h5py\n",
    "    with tf.Session() as sess:\n",
    "        #from notebooks_src.load_data.get_generator import get_generator\n",
    "\n",
    "        #gen=get_generator(\"tr\", batch_size=2)\n",
    "        y_true = tf.placeholder(tf.float32,shape=(5,15,5))\n",
    "        box = h5py.File(configs[\"tr_data_file\"])[\"boxes\"][323:328]\n",
    "        shapes = [(5, 6, 9, 54),\n",
    "                  (5, 3, 5, 36),\n",
    "                    (5, 96, 144, 36),\n",
    "                    (5, 24, 36, 54),\n",
    "                    (5, 12, 18, 54),\n",
    "                    (5, 48, 72, 54),\n",
    "                    (5, 1, 1, 36)]\n",
    "\n",
    "        box = box.astype(\"float32\")\n",
    "        y_preds = [tf.random_normal(mean=0.0,stddev=3.,shape=shape) for shape in shapes]\n",
    "        da_loss = compute_loss(y_true, y_preds)\n",
    "        \n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(\"/home/evan/hur-detect/src/debug_logs/try1\", sess.graph)\n",
    "        \n",
    "        loss_, summary = sess.run([da_loss,merged], feed_dict={y_true:box})\n",
    "        print loss_\n",
    "        writer.add_summary(summary,0)\n",
    "        writer.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    with tf.name_scope(\"test\"):\n",
    "        a = tf.constant(0)\n",
    "        tf.summary.scalar(\"a\", a)\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"/home/evan/hur-detect/src/debug_logs/try2\")\n",
    "    summary = sess.run(merged)\n",
    "    writer.add_summary(summary,0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.image.draw_bounding_boxes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
