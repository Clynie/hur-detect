{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ../../../notebooks_src/losses/utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "slim=tf.contrib.slim\n",
    "if __name__ == \"__main__\":\n",
    "    sys.path.append(\"../../../\")\n",
    "from notebooks_src.losses.utils import abs_smooth as smooth_L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ../../../notebooks_src/postprocessing/utils.ipynb\n",
      "importing Jupyter notebook from ../../../notebooks_src/box_processing/match.ipynb\n",
      "importing Jupyter notebook from ../../../notebooks_src/box_processing/tf_box_util.ipynb\n",
      "importing Jupyter notebook from ../../../notebooks_src/box_processing/make_anchors_orig.ipynb\n",
      "importing Jupyter notebook from ../../../notebooks_src/box_processing/encode.ipynb\n",
      "importing Jupyter notebook from ../../../notebooks_src/box_processing/unpack.ipynb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from notebooks_src.configs import configs\n",
    "from notebooks_src.postprocessing.utils import get_int_tensor_shape, sort_some_lists_of_tensors\n",
    "\n",
    "from notebooks_src.box_processing.match import match_boxes\n",
    "from notebooks_src.box_processing.encode import encode\n",
    "from notebooks_src.box_processing.unpack import unpack_net_output, split_box_class\n",
    "#from utils import ssd_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(y_true, y_preds):\n",
    "    '''y_true: the boxes Nx15x5 tensor\n",
    "       y_preds: a list of 7?  tensors of Nxfy x fx x k where k = 4*number of anchors + number_of_anchors*num_classes,\n",
    "       N is number of examples'''\n",
    "\n",
    "    bboxes, labels = split_box_class(y_true)\n",
    "    \n",
    "    encoded_boxes_dict, encoded_labels_dict = encode(bboxes,labels)\n",
    "\n",
    "    mask_dict,actual_gt_box_mask = match_boxes(bboxes)\n",
    "    \n",
    "    loc_dict, log_dict, pred_dict = unpack_net_output(y_preds)\n",
    "    loss = tf.constant(0.)\n",
    "\n",
    "    for fmap_size in encoded_boxes_dict.keys():\n",
    "        x_mask, tp_mask, num_matches = mask_dict[fmap_size]\n",
    "\n",
    "        tf.summary.scalar(name=\"num_positives_for_%i_%i\"%(fmap_size[0],fmap_size[1]),tensor=num_matches)\n",
    "\n",
    "#         with tf.name_scope(\"x_mask\"):\n",
    "#             batch_size = configs[\"batch_size\"]\n",
    "#             max_boxes = configs[\"num_max_boxes\"]\n",
    "#             num_anchors = 4\n",
    "#             for anch in range(num_anchors):\n",
    "#                 for gt_box in range(max_boxes):\n",
    "\n",
    "#                     x_im = tf.cast(tf.expand_dims(x_mask[:,:,:,anch,gt_box], axis=-1), tf.float32)\n",
    "#                     tf.summary.image(\"xmask_for_fmap_%i_%i_anchor_%i_gt_box_%i\"%(fmap_size[0],fmap_size[1],anch,gt_box),\n",
    "#                                      x_im)\n",
    "\n",
    "        encoded_boxes, encoded_labels, loc, log, pred = [dic[fmap_size] for dic in [encoded_boxes_dict,\n",
    "                                                          encoded_labels_dict,\n",
    "                                                           loc_dict, log_dict, pred_dict ]]\n",
    "\n",
    "\n",
    "        fmap_loss = calc_loss_one_layer(encoded_boxes, encoded_labels, x_mask, tp_mask,\n",
    "                                       num_matches,actual_gt_box_mask, loc, log, pred, fmap_size )\n",
    "\n",
    "        loss = loss + fmap_loss\n",
    "    return loss\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_loss_one_layer(encoded_boxes, encoded_labels, x_mask, tp_mask, num_matches,actual_gt_box_mask, loc, log, pred, fmap_size ):\n",
    "    loc_loss = calc_loc_loss(x_mask, encoded_boxes, loc)\n",
    "    cls_loss = calc_cls_loss(x_mask, encoded_labels,log, pred,actual_gt_box_mask, tp_mask, num_matches, fmap_size )\n",
    "    return (loc_loss + cls_loss) * (1./ num_matches) * (1./ configs[\"batch_size\"])\n",
    "\n",
    "def calc_loc_loss(x_mask, encoded_boxes, loc):\n",
    "    num_coords_in_a_box = 4\n",
    "    loc = tf.expand_dims(loc,axis=-1)\n",
    "    x_mask = tf.stack(num_coords_in_a_box*[x_mask], axis=-2)\n",
    "    loc_loss = smooth_L1(encoded_boxes - loc)\n",
    "    \n",
    "    # this gets rid of nans and infs that were encountered when having heights and widths of zero due to zeroing out -1's\n",
    "    #from the labels!\n",
    "    loc_loss = tf.boolean_mask(loc_loss, x_mask)\n",
    "    with tf.name_scope(\"loc_losses\"):\n",
    "        loc_loss = tf.reduce_sum(loc_loss)\n",
    "        tf.summary.scalar(\"loc_loss\", loc_loss)\n",
    "    return loc_loss\n",
    "\n",
    "def calc_cls_loss(x_mask, encoded_labels,log, pred,actual_gt_box_mask, tp_mask, num_matches, fmap_size ):\n",
    "    fmap_size\n",
    "    log = tf.stack(configs[\"num_max_boxes\"]*[log], axis=-2)\n",
    "    xent_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=log, labels=encoded_labels)\n",
    "    xent_loss = tf.boolean_mask(xent_loss, x_mask)\n",
    "    pos_xent = tf.reduce_sum(xent_loss)\n",
    "    \n",
    "    fp_mask = make_fp_mask(actual_gt_box_mask, tp_mask)\n",
    "    num_negs = tf.cast(configs[\"negative_ratio\"] * num_matches, dtype=tf.int32)\n",
    "\n",
    "    tf.summary.scalar(name=\"num_mined_negatives_for_%i_%i\"%(fmap_size[0],fmap_size[1]),tensor=num_negs)\n",
    "        \n",
    "    hard_neg_mask = make_hard_neg_mask(pred, fp_mask, num_negs, fmap_size)\n",
    "    \n",
    "    neg_encoded_labels = tf.where(hard_neg_mask, configs[\"num_classes\"]*tf.ones_like(encoded_labels), encoded_labels)\n",
    "    neg_xent_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=log, labels=neg_encoded_labels)\n",
    "    neg_xent_loss = tf.boolean_mask(neg_xent_loss, x_mask)\n",
    "    neg_xent = tf.reduce_sum(neg_xent_loss)\n",
    "    \n",
    "    with tf.name_scope(\"cls_losses\"):\n",
    "        tf.summary.scalar(\"pos_xent\",pos_xent)\n",
    "        tf.summary.scalar(\"neg_xent\", neg_xent)\n",
    "    return pos_xent + neg_xent\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_hard_neg_mask(pred, fp_mask, num_negs, fmap_size):\n",
    "    pred = tf.stack(15*[pred], axis=-2)\n",
    "    float_fp_mask = tf.cast(fp_mask, dtype=pred.dtype)\n",
    "\n",
    "    float_fp_mask = tf.expand_dims(float_fp_mask, axis=-1)\n",
    "    fp_pred = pred * float_fp_mask\n",
    "    last_axis = len(fp_pred.get_shape()) -1\n",
    "    # bg class is the last element of logits\n",
    "    _, bg_fp_pred = tf.split(fp_pred, num_or_size_splits=[configs[\"num_classes\"], 1], axis=last_axis)\n",
    "    flat_bg_fp_pred = tf.reshape(bg_fp_pred, [-1])\n",
    "    \n",
    "    num_total_negs = flat_bg_fp_pred.get_shape()[0]\n",
    "    tf.summary.scalar(name=\"num_total_negatives_for_%i_%i\"%(fmap_size[0],fmap_size[1]),tensor=num_total_negs)\n",
    "    k = tf.where(num_negs > num_total_negs, num_total_negs, num_negs)\n",
    "    bg_confs, inds = tf.nn.top_k(flat_bg_fp_pred,k=k)\n",
    "    min_bg_conf = bg_confs[-1]\n",
    "    hard_neg_mask = tf.greater_equal(bg_fp_pred, min_bg_conf)\n",
    "    hard_neg_mask = tf.squeeze(hard_neg_mask, axis=-1)\n",
    "    return hard_neg_mask\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_fp_mask(actual_gt_box_mask, tp_mask):\n",
    "    #tp_mask = tf.Print(data=[tf.reduce_sum(tf.cast(tp_mask, dtype=tf.int32))], input_=tp_mask, message=\"tpmask num nonzeros: \")\n",
    "    #is an actual box, but does not have an overlap > 0.5 with gt\n",
    "    fp_mask = tf.logical_and(actual_gt_box_mask, tf.logical_not(tp_mask))\n",
    "    fp_mask = tf.transpose(fp_mask, perm=(3,0,1,2,4))\n",
    "    #fp_mask = tf.Print(data=[tf.reduce_sum(tf.cast(fp_mask, dtype=tf.int32))], input_=fp_mask, message=\"fpmask num nonzeros: \")\n",
    "    return fp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.4437\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import h5py\n",
    "    with tf.Session() as sess:\n",
    "        #from notebooks_src.load_data.get_generator import get_generator\n",
    "\n",
    "        #gen=get_generator(\"tr\", batch_size=2)\n",
    "        y_true = tf.placeholder(tf.float32,shape=(5,15,5))\n",
    "        box = h5py.File(configs[\"tr_data_file\"])[\"boxes\"][323:328]\n",
    "        shapes = [(5, 6, 9, 54),\n",
    "                  (5, 3, 5, 36),\n",
    "                    (5, 96, 144, 36),\n",
    "                    (5, 24, 36, 54),\n",
    "                    (5, 12, 18, 54),\n",
    "                    (5, 48, 72, 54),\n",
    "                    (5, 1, 1, 36)]\n",
    "\n",
    "        y_preds = [tf.random_normal(mean=0.0,stddev=3.,shape=shape) for shape in shapes]\n",
    "\n",
    "        da_loss = compute_loss(y_true, y_preds)\n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(\"/home/evan/hur-detect/src/debug_logs/try1\", sess.graph)\n",
    "        loss_, summary = sess.run([da_loss,merged], feed_dict={y_true:box})\n",
    "        print loss_\n",
    "        writer.add_summary(summary,0)\n",
    "        writer.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    with tf.name_scope(\"test\"):\n",
    "        a = tf.constant(0)\n",
    "        tf.summary.scalar(\"a\", a)\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"/home/evan/hur-detect/src/debug_logs/try2\")\n",
    "    summary = sess.run(merged)\n",
    "    writer.add_summary(summary,0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
